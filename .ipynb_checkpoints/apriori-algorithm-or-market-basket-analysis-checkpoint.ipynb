{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "346634847c3c5e74c326fa2e927de776d6a6588f"
   },
   "source": [
    "# Market Basket Analysis using assocition rules \n",
    "\n",
    "<hr>\n",
    "\n",
    "**`In this Notebook we would learn`** \n",
    "\n",
    "1. What is Association Rule Learning?\n",
    "\n",
    "2. How Apriori works ?\n",
    "\n",
    "3. Implementing Apriori With Python - in 2 ways\n",
    "\n",
    "This notebook is made by <a href=\"https://www.linkedin.com/in/rocky-jagtiani-3b390649/\">Rocky Jagtiani</a> Head Content Development & Training at <a href=\"https://www.linkedin.com/company/suven-consultants-and-technology-pvt-ltd\"> Suven Consultants & Technology Pvt Ltd.</a> - <b>Training & Recruitment Company</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d83854725434ebf80b808a6b883d635ac3171cd1"
   },
   "source": [
    "**Problem Statement :**\n",
    "\n",
    "When we go grocery shopping, we often have a standard list of things to buy. Each shopper has a distinctive list, depending on oneâ€™s needs and preferences. A housewife might buy healthy ingredients for a family dinner, while a bachelor might buy beer and chips. Understanding these buying patterns can help to increase sales in several ways. If there is a pair of items, X and Y, that are frequently bought together:\n",
    "\n",
    "> Both X and Y can be placed on the same shelf, so that buyers of one item would be prompted to buy the other.\n",
    "\n",
    "> Promotional discounts could be applied to just one out of the two items.\n",
    "\n",
    "> Advertisements on X could be targeted at buyers who purchase Y.\n",
    "\n",
    "> X and Y could be combined into a new product, such as having Y in flavors of X.\n",
    "\n",
    "> **While we may know that certain items are frequently bought together, the question is, how do we uncover these associations?**\n",
    "\n",
    "*Besides increasing sales profits, association rules can also be used in other fields. In medical diagnosis for instance, understanding which symptoms tend to co-morbid can help to improve patient care and medicine prescription.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Association Rule Learning?\n",
    "--\n",
    "\n",
    "Association Rule Learning is rule-based learning for identifying the association between different variables in a database. **`One of the best and most popular examples of Association Rule Learning is the Market Basket Analysis`**. The problem analyses the association between various items that has the highest probability of being bought together by a customer.\n",
    "\n",
    "For example, the association rule, {onions, chicken masala} => {chicken} says that a person who has got both onions and chicken masala in his or her basket has a high probability of buying chicken also.\n",
    "\n",
    "\n",
    "Apriori Algorithm\n",
    "--\n",
    "\n",
    "The algorithm was first proposed in 1994 by Rakesh Agrawal and Ramakrishnan Srikant. Apriori algorithm finds the most frequent itemsets or elements in a transaction database and identifies association rules between the items just like the above-mentioned example.\n",
    "\n",
    "\n",
    "How Apriori works ?\n",
    "--\n",
    "\n",
    "To construct association rules between elements or items, the algorithm considers 3 important factors which are, support, confidence and lift. Each of these factors is explained as follows:\n",
    "\n",
    "**Support:**\n",
    "\n",
    "The support of item I is defined as the ratio between the number of transactions containing the item I by the total number of transactions expressed as :\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/403/0*pyOADkeaWyrVP2ft.png\" />\n",
    "\n",
    "<font color='darkgreen'><b>Support</b></font> indicates how popular an itemset is, as measured by the proportion of transactions in which an itemset appears. In Table 1 below, the support of {apple} is 4 out of 8, or 50%. Itemsets can also contain multiple items. For instance, the support of {apple, beer, rice} is 2 out of 8, or 25%.\n",
    "\n",
    "<img src='https://annalyzin.files.wordpress.com/2016/04/association-rule-support-table.png?w=503&h=447' />\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'><em>If you discover that sales of items beyond a certain proportion tend to have a significant impact on your profits, you might consider using that proportion as your support threshold. You may then identify itemsets with support values above this threshold as significant itemsets.</em></font>\n",
    "\n",
    "\n",
    "**Confidence:**\n",
    "\n",
    "This is measured by the proportion of transactions with item I1, in which item I2 also appears. The confidence between two items I1 and I2,  in a transaction is defined as the total number of transactions containing both items I1 and I2 divided by the total number of transactions containing I1. ( Assume I1 as X , I2 as Y )\n",
    "\n",
    "<img src='https://miro.medium.com/max/576/1*50GI4dR58MnhwBP9dw6nFQ.png' />\n",
    "\n",
    "<font color='darkgreen'><b>Confidence</b></font> says how likely item Y is purchased when item X is purchased, expressed as {X -> Y}. This is measured by the proportion of transactions with item X, in which item Y also appears. In Table 1, the confidence of {apple -> beer} is 3 out of 4, or 75%.\n",
    "\n",
    "<img src='https://annalyzin.files.wordpress.com/2016/03/association-rule-confidence-eqn.png?w=527&h=77' />\n",
    "\n",
    "<font color='red'><em>One drawback of the confidence measure is that it might misrepresent the importance of an association. This is because it only accounts for how popular apples are, but not beers. If beers are also very popular in general, there will be a higher chance that a transaction containing apples will also contain beers, thus inflating the confidence measure. To account for the base popularity of both constituent items, we use a third measure called lift. </em></font>\n",
    "\n",
    "\n",
    "**Lift:**\n",
    "\n",
    "Lift is the ratio between the confidence and support.\n",
    "\n",
    "<font color='darkgreen'><b>Lift</b></font> says how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is. In Table 1, the lift of {apple -> beer} is 1,which implies no association between items. A lift value greater than 1 means that item Y is likely to be bought if item X is bought, while a value less than 1 means that item Y is unlikely to be bought if item X is bought. ( *here X represents apple and Y represents beer* )\n",
    "\n",
    "<img src='https://annalyzin.files.wordpress.com/2016/03/association-rule-lift-eqn.png?w=566&h=80' />\n",
    "\n",
    "<hr>\n",
    "\n",
    "for **Extra Reading** : refer <a href='https://towardsdatascience.com/association-rules-2-aa9a77241654'> this </a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Apriori With Python - in 2 ways \n",
    "--\n",
    "\n",
    "<h3>In the <b><u>first way</u></b> , we use apyori package.</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8eeae1ae1fc473fc8d1f07a67981772ce4ec9eb5"
   },
   "outputs": [],
   "source": [
    "#External package need to install\n",
    "!pip install apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e54f39f9d95027f03de6a105807bf0c3b94a4d59"
   },
   "outputs": [],
   "source": [
    "#import all required packages..\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9010389e1814455f6cab002ee973558d720168f6"
   },
   "outputs": [],
   "source": [
    "#loading market basket dataset..\n",
    "\n",
    "df = pd.read_csv('../input/basket-optimisation/Market_Basket_Optimisation.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "636f8cfe210199e6b2731309d2b7d9166a950930"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f3dbd2a0d3eca79e3cb5d685248222a07f3a93e"
   },
   "outputs": [],
   "source": [
    "## Data Cleaning step\n",
    "\n",
    "# replacing empty value with 0.\n",
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "183803219cc64a347c36b435eff8ee2f757a8b33"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f4673e83dc22d65bcb5fc3bc752075d71c81891"
   },
   "outputs": [],
   "source": [
    "# Data Pre-processing step\n",
    "\n",
    "# for using aprori , need to convert data in list format..\n",
    "# transaction = [['apple','almonds'],['apple'],['banana','apple']]....\n",
    "\n",
    "transactions = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    transactions.append([str(df.values[i,j]) for j in range(0,20) if str(df.values[i,j])!='0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5866e48f904afd3b74b9cf8b8efe4506efd4e480"
   },
   "outputs": [],
   "source": [
    "## verifying - by printing the 0th transaction\n",
    "transactions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## verifying - by printing the 1st transaction\n",
    "transactions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62151154f73de9ea3be1284f3ee9b804e665c1b1"
   },
   "outputs": [],
   "source": [
    "# Call apriori function which requires minimum support, confidance and lift, min length is combination of item default is 2\".\n",
    "rules = apriori(transactions, min_support=0.003, min_confidance=0.2, min_lift=3, min_length=2)\n",
    "\n",
    "## min_support = 0.003 -> means selecting items with min support of 0.3%\n",
    "## min_confidance = 0.2 -> means min confidance of 20% \n",
    "## min_lift = 3  \n",
    "## min_length = 2 -> means no. of items in the transaction should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a31b87cb69a410c8db756f941053ce0d76848fe"
   },
   "outputs": [],
   "source": [
    "#it generates a set of rules in a generator file...\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "3ae063ad3f409626e1fed6b2432d159e6a3d7e7c"
   },
   "outputs": [],
   "source": [
    "# all rules need to be converted in a list..\n",
    "Results = list(rules)\n",
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6d1bdb35db052eac0fdce71a46a313773bddd82"
   },
   "outputs": [],
   "source": [
    "# convert result in a dataframe for further operation...\n",
    "df_results = pd.DataFrame(Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6df62c0506f46b7c5aba205dd12e59ad62c143df"
   },
   "outputs": [],
   "source": [
    "# as we see \"order_statistics\" , is itself a list so need to be converted in proper format..\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf7711b724f2906a0c73cc29f683b5a5734b0c31"
   },
   "outputs": [],
   "source": [
    "# keep support in a separate data frame so we can use later.. \n",
    "support = df_results.support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f48bb932b0266da190da81202ea953f97fd1d60"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "convert orderstatistic in a proper format.\n",
    "order statistic has lhs => rhs as well rhs => lhs \n",
    "we can choose any one for convience.\n",
    "Let's choose first one which is 'df_results['ordered_statistics'][i][0]'\n",
    "''' \n",
    "\n",
    "#all four empty list which will contain lhs, rhs, confidance and lift respectively.\n",
    "first_values = []\n",
    "second_values = []\n",
    "third_values = []\n",
    "fourth_value = []\n",
    "\n",
    "# loop number of rows time and append 1 by 1 value in a separate list.. \n",
    "# first and second element was frozenset which need to be converted in list..\n",
    "for i in range(df_results.shape[0]):\n",
    "    single_list = df_results['ordered_statistics'][i][0]\n",
    "    first_values.append(list(single_list[0]))\n",
    "    second_values.append(list(single_list[1]))\n",
    "    third_values.append(single_list[2])\n",
    "    fourth_value.append(single_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bce8d56076e72034cd2cea11f0f6b81658f38a9"
   },
   "outputs": [],
   "source": [
    "# convert all four list into dataframe for further operation..\n",
    "lhs = pd.DataFrame(first_values)\n",
    "rhs = pd.DataFrame(second_values)\n",
    "\n",
    "confidance=pd.DataFrame(third_values,columns=['Confidance'])\n",
    "\n",
    "lift=pd.DataFrame(fourth_value,columns=['lift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "5a46c51eb0ca2b82a4f8967947ec70e76f46c052"
   },
   "outputs": [],
   "source": [
    "# concat all list together in a single dataframe\n",
    "df_final = pd.concat([lhs,rhs,support,confidance,lift], axis=1)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88498a6a76fdefd003066a660005fc45d6f83956"
   },
   "outputs": [],
   "source": [
    "'''\n",
    " we have some of place only 1 item in lhs and some place 3 or more so we need to a proper represenation for User to understand. \n",
    " replacing none with ' ' and combining three column's in 1 \n",
    " example : coffee,none,none is converted to coffee, ,\n",
    "'''\n",
    "df_final.fillna(value=' ', inplace=True)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63dddfe92400416e69c50d7fb1d9f0d628dba075"
   },
   "outputs": [],
   "source": [
    "#set column name\n",
    "df_final.columns = ['lhs',1,'rhs',2,3,'support','confidance','lift']\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "263d8e70087aa19d9f7424ff33171aba499aa793"
   },
   "outputs": [],
   "source": [
    "# add all three column to lhs itemset only\n",
    "df_final['lhs'] = df_final['lhs'] + str(\", \") + df_final[1]\n",
    "\n",
    "df_final['rhs'] = df_final['rhs']+str(\", \")+df_final[2] + str(\", \") + df_final[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6927d65223d04325ceb67b18d7a5f3dd17970780"
   },
   "outputs": [],
   "source": [
    "#drop columns 1,2 and 3 because now we already appended to lhs column.\n",
    "\n",
    "df_final.drop(columns=[1,2,3],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "979b380863e1d375732a2049b2a93b7d6c8869b7"
   },
   "outputs": [],
   "source": [
    "#this is final output. You can sort based on the support lift and confidance..\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing top 10 items, based on lift.  Sorting in desc order\n",
    "df_final.sort_values('lift', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37fad95b716a1db144a722358f8319148af5b414"
   },
   "source": [
    "Other way of doing Apriori in Python\n",
    "--\n",
    "\n",
    "<h3>In the <u>Second way</u> we would use <b>mlxtend.frequent_patterns</b> package.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b81a5948df0ec288269de14bfc42d5eed531f88"
   },
   "source": [
    "<font color='red'><b>Why we doing it in this way</b></font> - \n",
    "\n",
    "1.  **Limitation of first approach** was need to convert data in a list fomat. In real life a store has many thousands of transactations hence it is computationally expensive.\n",
    "\n",
    "2. `Apyori package is outdated`. \n",
    "\n",
    "3. Results are coming in less friendly format so their is a need for pre-processing.\n",
    "\n",
    "4. Instead lets use **`mlxtend`** package. It generate frequent itemset and association rules both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bae69fde5cd7de2c715f98244818b697d7f65f67"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "load apriori and association modules from mlxtend.frequent_patterns \n",
    "Used different dataset because mlxtend need data in below format. \n",
    "\n",
    "    transaction_name    apple banana grapes\n",
    "transaction  1            0     1      1\n",
    "             2            1     0      1  \n",
    "             3            1     0      0\n",
    "             4            0     1      0\n",
    "             \n",
    " we could have used above data as well but need to perform operation to bring in this format instead of that used seperate data only.            \n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "df1 = pd.read_csv('../input/ecommerce-dataset/data-2.csv', encoding=\"ISO-8859-1\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b869156690e357ab9d72778e3ba69552e6a4632f"
   },
   "outputs": [],
   "source": [
    "# data has many country choose any one for check.\n",
    "df1.Country.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64e8bc19acc300699e2c4e08c956ca53531a141c"
   },
   "outputs": [],
   "source": [
    "# using only France country data for now;  can check for other as well..\n",
    "df1 = df1[df1.Country == 'France']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "556fc5d1c636d5fe5446616f0b62afabe173fc02"
   },
   "outputs": [],
   "source": [
    "# some spaces are there in description; need to remove else later operation it will create problem..\n",
    "df1['Description'] = df1['Description'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d64fedb4166b8e83abf455789a57c950f36cf943"
   },
   "outputs": [],
   "source": [
    "#some of transaction quantity is negative which can not be possible remove them.\n",
    "df1 = df1[df1.Quantity >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12361137c1bc7bc5e559262cbe67528941cc23f5"
   },
   "outputs": [],
   "source": [
    "#df1[df1.Country == 'France'].head(10)\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f8ba588c00ed64a579e872abebd7c0b9f8088e8"
   },
   "outputs": [],
   "source": [
    "# convert data in format which is required \n",
    "# converting using pivot table and Quantity sum as values. fill 0 if any nan values\n",
    "\n",
    "basket = pd.pivot_table(data=df1,index='InvoiceNo',columns='Description',values='Quantity', aggfunc='sum',fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32adc08933fe8c3d961ec042304ee56f57660e35"
   },
   "outputs": [],
   "source": [
    "basket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74c18efa837907b4a1a6d99ad7163812bb66c5f0"
   },
   "outputs": [],
   "source": [
    "#this to check correctness after binning it to 1 ..\n",
    "basket['10 COLOUR SPACEBOY PEN'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b849345a1c6b4ba2c8af6ccd5b8259c52d93ca2e"
   },
   "outputs": [],
   "source": [
    "# we dont need quantity sum \n",
    "# we need either has taken or not \n",
    "# so if user has taken that item mark as 1 else mark as 0.\n",
    "\n",
    "def convert_into_binary(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b65902345422094789cd85bbdeb2d2c8f22d4a58"
   },
   "outputs": [],
   "source": [
    "basket_sets = basket.applymap(convert_into_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a9568d732d9a1ee43921af8b1c1360dd11a82ff"
   },
   "outputs": [],
   "source": [
    "# check : has quantity now converted to 1 or 0.\n",
    "basket_sets['10 COLOUR SPACEBOY PEN'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8913f5904549dbd50f24f9fbc9949daf59b29596"
   },
   "outputs": [],
   "source": [
    "# remove postage item as it is just a seal which almost all transaction contains. \n",
    "print(basket_sets['POSTAGE'].head())\n",
    "\n",
    "basket_sets.drop(columns=['POSTAGE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7eb9a2960368623c466713fcafc19389997a1b74"
   },
   "outputs": [],
   "source": [
    "# call apriori function and pass minimum support here we are passing 7%. \n",
    "# means 7 times in total number of transaction the item should be present.\n",
    "frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "354f06be1525ac12a6c7f57f561b8cabd8487b5f"
   },
   "outputs": [],
   "source": [
    "#it will generate frequent itemsets \n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating frequent itemsets from a list of items - \n",
    "--\n",
    "\n",
    "First step in generation of association rules is to get all the frequent itemsets on which binary partitions can be performed to get the antecedent and the consequent. \n",
    "\n",
    "\n",
    "<img src='https://miro.medium.com/max/576/1*NoUFBxsokdnw3wrYyVXkgw.png' />\n",
    "\n",
    "\n",
    "`For example`, if there are 6 items {Bread, Butter, Egg, Milk, Notebook, Toothbrush} on all the transactions combined, itemsets will look like {Bread}, {Butter}, {Bread, Notebook}, {Milk, Toothbrush}, {Milk, Egg, Vegetables} etc. Size of an itemset can vary from one to the total number of items that we have. Now, we seek only frequent itemsets from this and not all so as to put a check on the number of total itemsets generated.\n",
    "\n",
    "<img src='https://miro.medium.com/max/256/1*gZujUCrD9nanRr-ETBFtbg.png' />\n",
    "\n",
    "<br>\n",
    "\n",
    "Frequent itemsets are the ones which occur at least a minimum number of times in the transactions. Technically, these are the itemsets for which support value (fraction of transactions containing the itemset) is above a **minimum threshold â€” min_support. We have kept min_support=0.07 in our above code**.\n",
    "\n",
    "So, {Bread, Notebook} might not be a frequent itemset if it occurs only 2 times out of 100 transactions and (2/100) = 0.02 falls below the value of min_support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17c7b926a354831f08a23edfb29c4a0aee7a6a2c"
   },
   "outputs": [],
   "source": [
    "# We would apply association rules on frequent itemset. \n",
    "# here we are setting based on lift and keeping minimum lift as 1\n",
    "\n",
    "rules_mlxtend = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "rules_mlxtend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating all possible rules from the frequent itemsets\n",
    "--\n",
    "\n",
    "Once the frequent itemsets are generated, identifying rules out of them is comparatively less taxing. Rules are formed by binary partition of each itemset. If {Bread,Egg,Milk,Butter} is the frequent itemset, candidate rules will look like:\n",
    "\n",
    "(Egg, Milk, Butter â†’ Bread), (Bread, Milk, Butter â†’ Egg), (Bread, Egg â†’ Milk, Butter), (Egg, Milk â†’ Bread, Butter), (Butterâ†’ Bread, Egg, Milk)\n",
    "\n",
    "From a list of all possible candidate rules, we aim to identify rules that fall above a minimum threshold level (like min_confidence or min_lift).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0eb5af08f401c56026c61812b1c7b3a736065ce8"
   },
   "outputs": [],
   "source": [
    "# rules_mlxtend.rename(columns={'antecedents':'lhs','consequents':'rhs'})\n",
    "\n",
    "# as based business use case we can sort based on confidance and lift.\n",
    "rules_mlxtend[ (rules_mlxtend['lift'] >= 4) & (rules_mlxtend['confidence'] >= 0.8) ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the above data ?\n",
    "--\n",
    "\n",
    "> antecedents and consequents  -> The IF component of an association rule is known as the antecedent. The THEN component is known as the consequent. The antecedent and the consequent are disjoint; they have no items in common.\n",
    "\n",
    "> Example of antecedents and consequents -> Consider antecedents as X and consequents as Y.  \n",
    "\n",
    "<img src='https://miro.medium.com/max/576/1*NoUFBxsokdnw3wrYyVXkgw.png' />\n",
    "\n",
    "> antecedent support -> This measure gives an idea of how frequent antecedent is in all the transactions. Like (ALARM CLOCK BAKELIKE RED) is present in 9.4% of the transactions. \t\n",
    "\n",
    "> consequent support -> This measure gives an idea of how frequent consequent is in all the transactions. Like (ALARM CLOCK BAKELIKE GREEN) is present in 9.6% of the transactions. \n",
    "\n",
    "> support -> This measure gives an idea of how frequent `ItemSet` is in all the transactions. Like {ALARM CLOCK BAKELIKE RED,ALARM CLOCK BAKELIKE GREEN} is present in 7.9% of the transactions. \n",
    "\n",
    "> confidance -> This measure defines the likeliness of occurrence of consequent on the cart given that the cart already has the antecedents. So {ALARM CLOCK BAKELIKE RED} -> {ALARM CLOCK BAKELIKE GREEN} as a confidence of 83%. In simple words their is an 83% chance of finding {ALARM CLOCK BAKELIKE GREEN} , if the cart contains {ALARM CLOCK BAKELIKE RED}.\n",
    "\n",
    "> lift -> This measure defines the likeliness of occurrence of consequent on the cart given that the cart already has the antecedent, but controlling the popularity of consequent. So lift of {ALARM CLOCK BAKELIKE GREEN} w.r.t {ALARM CLOCK BAKELIKE RED} is 8.64. Which is quite good. Any lift value > 1 implies that the Association rule is worth considering.  [ Reference : https://en.wikipedia.org/wiki/Lift_(data_mining)]\n",
    "\n",
    "> leverage -> leverage(X -> Y) = P(X and Y) - (P(X)P(Y))\n",
    "\n",
    "*Leverage measures the difference of X and Y appearing together in the data set and what would be expected if X and Y where statistically dependent. The rational in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sells.*  [Reference : https://michael.hahsler.net/research/recommender/associationrules.html]\n",
    "\n",
    "> conviction -> Ignore this parameter.  Not much of use in most situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect with the Author of the Notebook \n",
    "--\n",
    "\n",
    "[ here https://linkedin.com/in/rocky-jagtiani-3b390649/ ] \n",
    "\n",
    "or \n",
    "\n",
    "[ here https://datascience.suvenconsultants.com/ ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
